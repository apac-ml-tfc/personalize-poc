{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Input Data for Amazon Personalize\n",
    "\n",
    "In this notebook, we'll work through **selecting and preparing** historical training data for Amazon Personalize - uploading the prepared files to Amazon S3.\n",
    "\n",
    "> ⚠️ We assume you've *already* set up an S3 bucket and IAM execution role for Personalize to access the bucket (either in the AWS console or by running notebook [00_Environment_Setup.ipynb](00_Environment_Setup.ipynb)) and `%store`d this information as we did in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon Personalize provides 3 types of \"recipe\" (algorithm), for solving different recommendation tasks. Some categories include multiple recipes, as follows:\n",
    "\n",
    "1. [**User Personalization**](https://docs.aws.amazon.com/personalize/latest/dg/user-personalization-recipes.html) - Recommend relevant items for a given user (with some business rule filtering, if required)\n",
    "    - The [**User-Personalization**](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-new-item-USER_PERSONALIZATION.html) recipe is the most fully-featured and typically **recommended for all use-cases in this category**\n",
    "    - The legacy **HRNN-...** recipes in this category should normally be substituted for User-Personalization\n",
    "    - The [**Popularity-Count**](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-popularity.html) is a useful **baseline** recipe for contextualizing performance metrics: e.g. \"What metrics would I get *just by always recommending the most popular items*)\n",
    "2. [**Personalized Ranking**](https://docs.aws.amazon.com/personalize/latest/dg/personalized-ranking-recipes.html) - Prioritize relevant items **from a provided shortlist** for a given user (for supplying lists, rather than applying filter rules)\n",
    "3. [**Related Items**](https://docs.aws.amazon.com/personalize/latest/dg/related-items-recipes.html) - Recommend relevant items for a given **item** in context (e.g. \"customers also bought\")\n",
    "\n",
    "No matter the use case, the algorithms all share a base of learning on **user-item-interaction data** - a set of *events* defined by 3 core required attributes:\n",
    "\n",
    "1. **User_ID** - The user who interacted\n",
    "2. **Item_ID** - The item the user interacted with\n",
    "3. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "Personalize also defines certain optional event attributes you can add:\n",
    "\n",
    "4. **Event_Type** - Categorical label of an event (browse, purchased, rated, etc).\n",
    "5. **Event_Value** - A value corresponding to the event type that occurred. As a best practice, we generally aim for normalized values between 0 and 1 to be comparable between event types. For example, if there are three phases to complete a transaction (clicked, added-to-cart, and purchased), then there would be an event_value for each phase as 0.33, 0.66, and 1.0 respectively.\n",
    "\n",
    "...and users can also add **custom interaction attributes** which some recipes can include in the generated model: E.g. location, device type, etc.\n",
    "\n",
    "In addition to this core **'Interactions' dataset**, we may *optionally* provide:\n",
    "\n",
    "- **Users metadata** - a table mapping each `USER_ID` to additional custom attributes such as gender, age group, etc.\n",
    "- **Items metadata** - a table mapping each `ITEM_ID` to additional custom attributes such as brand, category, etc.\n",
    "\n",
    "More detail on the schema and requirements for Amazon Personalize training data is given in the [Datasets and Schemas](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html) and [Preparing and Importing Data](https://docs.aws.amazon.com/personalize/latest/dg/data-prep.html) sections of the Amazon Personalize Developer Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Data Source\n",
    "\n",
    "> ⚠️ **Remember:** Only the *Interactions* dataset is mandatory, and all recipes in Amazon Personalize at the time of writing use this as their primary learning source - with secondary input from other metadata.\n",
    ">\n",
    "> See the official [Amazon Personalize Cheat Sheet](https://github.com/aws-samples/amazon-personalize-samples/blob/master/PersonalizeCheatSheet2.0.md) for more guidance on **what use-cases Amazon Personalize is a good fit for**, and where you might consider alternative techniques instead.\n",
    "\n",
    "Many use-cases *do* generate this kind of interaction history data - for example:\n",
    "\n",
    "1. Video-on-demand applications\n",
    "1. E-commerce platforms\n",
    "1. Social media aggregators / platforms\n",
    "\n",
    "...And we should be reasonably well-placed as long as our use case meets some basic criteria like:\n",
    "\n",
    "- Authenticated users\n",
    "- At least 50 unique users\n",
    "- At least 100 unique items\n",
    "- Several (and prefarably at least 2 dozen) interactions for each user\n",
    "\n",
    "However, your historical data will typically not arrive in a perfect form & compatible schema for Personalize - so our first task will be to procure and **re-structure** the training data.\n",
    "\n",
    "In this example, we'll use the [**MovieLens dataset**](https://grouplens.org/datasets/movielens/) to explore the service: A set of *movie reviews*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Data (MovieLens Example)\n",
    "\n",
    "The full MovieLens dataset includes over 25 million interactions (reviews) and a rich collection of metadata for items (movies).\n",
    "\n",
    "There's also a smaller published extract of the dataset available, which we'll use by default here to shorten training times while still demonstrating the same capabilities. Set `USE_FULL_MOVIELENS` to `True` below if you'd like to use the full dataset instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FULL_MOVIELENS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"poc_data\"\n",
    "%store data_dir\n",
    "\n",
    "!mkdir -p $data_dir\n",
    "\n",
    "# Download and extract the dataset:\n",
    "if not USE_FULL_MOVIELENS:\n",
    "    !cd $data_dir && wget -N http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "    !cd $data_dir && unzip -o ml-latest-small.zip\n",
    "    dataset_dir = data_dir + \"/ml-latest-small/\"\n",
    "else:\n",
    "    !cd $data_dir && wget -N http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "    !cd $data_dir && unzip -o ml-25m.zip\n",
    "    dataset_dir = data_dir + \"/ml-25m/\"\n",
    "%store dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data files you've downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present not much is known except that we have a few CSVs and a readme. Let's output the readme to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $dataset_dir/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the README, we see there is a file `ratings.csv` that should form the basis of our **interactions** data - after all, rating a film definitely is a form of interacting with it!\n",
    "\n",
    "The dataset also has some genre information as some movie genome data, which looks like a good source of **item metadata**.\n",
    "\n",
    "As a publicly released dataset, there's not really any useful **user metadata** available here for us to model with - so we'll largely ignore this feature of Personalize in our example... But you can take the item metadata process as a good guide for your own experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Interactions Data\n",
    "\n",
    "To get started exploring our data, we'll first import a few useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import pandas as pd  # DataFrame (table) manipulation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'd like to load our data and check the format, scope, and any gaps - to understand what might need adapting for Amazon Personalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(dataset_dir + \"/ratings.csv\")\n",
    "\n",
    "print(original_data.info())\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, you can see that there are a total of (25,000,095 for full 100836 for small) entries in the dataset, with 4 columns, and each cell has been interpreted as int64 format, except for the rating which has been loaded as float64.\n",
    "\n",
    "We see that there are no null entries in any column (non-null counts all match), but may also be good to check the extent & basic statistics of the columns in case there's anything unexpected there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ranges seem to be generally healthy for our `userId`, `movieId` and `rating` columns - and the inferred int64 & float64 formats clearly seem suitable for these fields.\n",
    "\n",
    "However, we need to dive deeper to understand the timestamps in the data. Amazon Personalize [requires](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-formatting.html#timestamp-data) timestamps in [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) format.\n",
    "\n",
    "Currently, the timestamp values are not human-readable. So let's grab an arbitrary timestamp value and check whether it seems consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_time_stamp = original_data.iloc[50][\"timestamp\"]\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This date seems to be within expected range, so we can continue formatting the rest of the data.\n",
    "\n",
    "The correspondence is pretty clear from `userId` to `USER_ID`, and from `movieId` to `ITEM_ID`... But what about `rating`?\n",
    "\n",
    "We can define a single `EVENT_TYPE` of \"review\", and use rating as our `EVENT_VALUE` field: Which will allow us to **threshold filter** our dataset to build models that consider only higher-rating events (e.g. to build a model more likely to recommend movies the user will actually *like*, not just what they're likely to review).\n",
    "\n",
    "...So the only data preparation we'll need to do in this example is to rename our columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = original_data.copy()\n",
    "interactions_df[\"EVENT_TYPE\"] = \"review\"\n",
    "interactions_df.rename(\n",
    "    columns={\n",
    "        \"userId\": \"USER_ID\",\n",
    "        \"movieId\": \"ITEM_ID\",\n",
    "        \"rating\": \"EVENT_VALUE\",\n",
    "        \"timestamp\": \"TIMESTAMP\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "print(interactions_df.info())\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import this dataset to Amazon Personalize, we'll need to save it in **CSV format** on **Amazon S3**, so first let's save the file to a CSV here on our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_path = f\"{data_dir}/{interactions_filename}\"\n",
    "%store interactions_path\n",
    "\n",
    "interactions_df.to_csv(interactions_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And then upload this file to the bucket we prepared in [00_Environment_Setup.ipynb](00_Environment_Setup.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the below line with e.g. region = \"ap-southeast-1\" if you didn't run notebook 0\n",
    "%store -r region\n",
    "assert isinstance(region, str), \"`region` must be a region name string e.g. 'us-east-1'\"\n",
    "\n",
    "# Replace the below line with e.g. bucket_name = \"DOC-EXAMPLE-BUCKET\" if you didn't run notebook 0\n",
    "%store -r bucket_name\n",
    "assert isinstance(bucket_name, str), \"`bucket_name` must be a data bucket name string\"\n",
    "\n",
    "session = boto3.Session(region_name=region)\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file to S3:\n",
    "s3.Bucket(bucket_name).Object(interactions_path).upload_file(interactions_path)\n",
    "interactions_s3uri = f\"s3://{bucket_name}/{interactions_path}\"\n",
    "%store interactions_s3uri\n",
    "print(f\"Uploaded interactions to {interactions_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - now our only *mandatory* dataset is ready to go! But, to try and improve our model, let's see if we can also prepare an item metadata file from the 'movies.csv' provided in the source dataset:\n",
    "\n",
    "## Preparing (Item) Metadata\n",
    "\n",
    "As before, we'll start out by loading and exploring the source data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_items = pd.read_csv(dataset_dir + \"/movies.csv\")\n",
    "\n",
    "print(original_items.info())\n",
    "original_items.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, `movieId` is the only numeric column and so the only one we can generate summary statistics for. we also see `title` (A string title which appears to include the year of the film in brackets at the end) and `genres` - a bar-separated list of multiple genre tags for each film.\n",
    "\n",
    "Amazon Personalize is already able to consume multi-categorical data in this bar-separated format, so we can use this `GENRES` field as-is.\n",
    "\n",
    "Let's:\n",
    "\n",
    "- Rename the `movieId` column to `ITEM_ID` in line with Amazon Personalize's required schema\n",
    "- Rename our other columns to uppercase for consistency\n",
    "- Try using a [regular expression](https://docs.python.org/3/library/re.html#module-re) to **extract the year** from the title field\n",
    "\n",
    "with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = original_items.copy()\n",
    "items_df.rename(\n",
    "    columns={\n",
    "        \"movieId\": \"ITEM_ID\",\n",
    "        \"title\": \"TITLE\",\n",
    "        \"genres\": \"GENRES\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "items_df[\"YEAR\"] = items_df[\"TITLE\"].str.extract(r\".*(\\d{4})\")\n",
    "items_df[\"YEAR\"] = pd.to_numeric(items_df[\"YEAR\"]).astype(\"Int64\")\n",
    "\n",
    "print(items_df.info())\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have gone pretty well overall - but we can see from the summary statistics there are a small proportion of null values in our new `YEAR` column: Let's quickly check whether these gaps make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df[items_df[\"YEAR\"].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples seem to make sense as the relevant titles don't obviously list a year.\n",
    "\n",
    "The proportion of missing values in the `YEAR` field is quite small, and the age of each movie is likely to be usefully descriptive - so we'll keep and use this field.\n",
    "\n",
    "However, at this time the `TITLE` field is pretty useless to us from a modelling perspective, as today Amazon Personalize can only interpret string fields as category identifiers, not use natural language features mentioned within them.\n",
    "\n",
    "...So as a final transformation we'll drop the `TITLE` column, and then our item metadata will be ready to load for modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.drop(\"TITLE\", axis=1, inplace=True)\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the interactions data, we now just need to save this dataset to CSV format and upload to our Amazon S3 Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_filename = \"item-meta.csv\"\n",
    "items_path = f\"{data_dir}/{items_filename}\"\n",
    "%store items_path\n",
    "\n",
    "items_df.to_csv(items_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file to S3:\n",
    "s3.Bucket(bucket_name).Object(items_path).upload_file(items_path)\n",
    "items_s3uri = f\"s3://{bucket_name}/{items_path}\"\n",
    "%store items_s3uri\n",
    "print(f\"Uploaded item metadata to {items_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All set!\n",
    "\n",
    "We've now prepared interaction and item metadata in a compatible format for Amazon Personalize, and uploaded it to Amazon S3 ready for import.\n",
    "\n",
    "In the next notebook we'll start actually using the Personalize service, and you have two choices:\n",
    "\n",
    "- Follow along in the **AWS Console** with the instructions and screenshots in [02a_Importing_Data_(Console).ipynb](02a_Importing_Data_(Console).ipynb), *OR*\n",
    "- Run the same steps in code with the **AWS SDK for Python (Boto3)** by following [02b_Importing_Data_(Python_SDK).ipynb](02b_Importing_Data_(Python_SDK).ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
