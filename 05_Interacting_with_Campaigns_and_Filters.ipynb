{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Campaigns and Filters\n",
    "\n",
    "In this notebook, we'll **use** our deployed recommendation models - querying them for recommendations and sending **feedback** to update the model state.\n",
    "\n",
    "⚠️ You'll need to already have run the previous notebooks in this series to set up your environment and deploy **campaigns** (endpoints) in Amazon Personalize, including **waiting for your campaigns to become active**\n",
    "\n",
    "Before we start, we'll here:\n",
    "\n",
    "- Import the libraries this notebook will use\n",
    "- Load the variables saved from previous steps\n",
    "- Connect to the relevant AWS services as we have before for IAM and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid  # For generating random IDs\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import pandas as pd  # DataFrame (table) manipulation tools\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Local Dependencies:\n",
    "import util  # Small tool to print progress spinner\n",
    "\n",
    "# import time\n",
    "# from time import sleep\n",
    "# import random\n",
    "\n",
    "# Reload saved variables:\n",
    "%store -r\n",
    "\n",
    "# Connect to AWS services:\n",
    "personalize = boto3.client(\"personalize\")  # We've used these management APIs before\n",
    "personalize_events = boto3.client(\"personalize-events\")  # Note this new one!\n",
    "personalize_runtime = boto3.client(\"personalize-runtime\")  # And this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Once a *campaign* is deployed, we have a private, real-time API ready to serve recommendation requests.\n",
    "\n",
    "Just like other similar AWS services:\n",
    "\n",
    "- We'll typically use this API via the AWS **SDKs for whatever language our application uses** (e.g. [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize-runtime.html) for Python, the [AWS SDK for JavaScript](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/PersonalizeRuntime.html) and so on)... not least because these simplify request signing/security for us.\n",
    "- Access is controlled by [AWS IAM](https://aws.amazon.com/iam/) - so we assume you're running this notebook in an environment (e.g. a SageMaker noteobook) with credentials (e.g. an execution role) authorized to interact with your Amazon Personalize resources (for example the IAM `AmazonPersonalizeFullAccess` policy).\n",
    "\n",
    "We'll start by loading up our movie metadata, which will allow us to associate returned movie IDs to their titles, and make results later a bit more human-readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = pd.read_csv(dataset_dir + \"/movies.csv\", index_col=\"movieId\")[[\"title\"]].rename(\n",
    "    columns={ \"title\": \"TITLE\" },\n",
    ")\n",
    "titles_df.head()\n",
    "\n",
    "items_df = pd.read_csv(items_path, index_col=\"ITEM_ID\", dtype={ \"YEAR\": \"Int64\" })\n",
    "\n",
    "items_df = items_df.join(titles_df)\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `ITEM_ID` as the **index** of this dataframe, we've made it very simple to look up movies using the `loc[]` operator - either one-by-one or in bulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.loc[[2, 3, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a simple utility function to retrieve the full ARN for a given campaign by name - to make it easy for you to update this code in case you chose different names for your deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaign_arn_by_name(campaign_name):\n",
    "    campaigns = personalize.list_campaigns()[\"campaigns\"]\n",
    "    try:\n",
    "        return next(filter(\n",
    "            lambda c: c[\"name\"] == campaign_name,\n",
    "            campaigns,\n",
    "        ))[\"campaignArn\"]\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Campaign '{}' not found! Got:\\n- {}\".format(\n",
    "            campaign_name,\n",
    "            \"\\n- \".join(map(lambda c: c[\"name\"], campaigns))\n",
    "        ))\n",
    "\n",
    "# For example:\n",
    "get_campaign_arn_by_name(\"personalize-movielens-sims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Recommendations: Similar Items\n",
    "\n",
    "Querying recommendations uses the `personalize-runtime` service, rather than the standard `personalize` service we've used previously for management operations (such as training solutions, deploying campaigns, and so on).\n",
    "\n",
    "Our `SIMS` campaign recommends \"similar\" items (in terms of the users who watch/click/buy them, not the item metadata) - so it takes an item ID as input.\n",
    "\n",
    "Item ID 1 from before was \"Toy Story (1995)\": Let's see what other films Toy Story reviewers might like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_campaign_arn = get_campaign_arn_by_name(\"personalize-movielens-sims\")\n",
    "\n",
    "sims_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn=sims_campaign_arn,\n",
    "    itemId=str(1),\n",
    "    numResults=10,\n",
    ")\n",
    "\n",
    "sims_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! But doesn't tell us much by itself. Let's map that raw result back to our items table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recs_to_dataframe(item_list):\n",
    "    recs_df = pd.DataFrame(item_list).rename(columns={ \"itemId\": \"ITEM_ID\" })\n",
    "    recs_df[\"ITEM_ID\"] = pd.to_numeric(recs_df[\"ITEM_ID\"]).astype(\"Int64\")\n",
    "    recs_df.set_index(\"ITEM_ID\", inplace=True)\n",
    "    return recs_df.join(items_df)\n",
    "\n",
    "recs_to_dataframe(sims_response[\"itemList\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more informative! In our test (your exact results may vary), Toy Story 2 came out top of the list - suggesting that users who reviewed (and liked) the original are very likely to enjoy the sequel... Seems to make sense!\n",
    "\n",
    "...But what do we see if we query similar items for a few movies at random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_items = items_df.sample(5)\n",
    "\n",
    "random_sims = {}\n",
    "n_recs = 10\n",
    "\n",
    "for item_id, item_meta in random_items.iterrows():\n",
    "    item_sims = recs_to_dataframe(\n",
    "        personalize_runtime.get_recommendations(\n",
    "            campaignArn=sims_campaign_arn,\n",
    "            itemId=str(item_id),\n",
    "            numResults=n_recs,\n",
    "        )[\"itemList\"]\n",
    "    )\n",
    "    random_sims[item_meta[\"TITLE\"]] = (\n",
    "        # Need to pad the results to n_recs because some movies might return fewer:\n",
    "        item_sims[\"TITLE\"].to_list() + (n_recs * [None])\n",
    "    )[:n_recs]\n",
    "\n",
    "random_sims = pd.DataFrame(random_sims)\n",
    "random_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably found that **a lot of the results look the same** (Hopefully not all of them - this is more likely with a smaller # of interactions, which may be more common with the small MovieLens subset).\n",
    "\n",
    "Why is this? Well, movies that have been **watched by a lot of users** will show up in the co-occurrence/\"similar\" sets for more movie IDs!\n",
    "\n",
    "This goes to show that evaluation metrics should not be the only thing we rely on when evaluating our solution version. So what can we do about it?\n",
    "\n",
    "This is a good time to revisit the **hyperparameters** of the Personalize recipes. The SIMS recipe has a `popularity_discount_factor` hyperparameter (see [documentation](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-sims.html)). Leveraging this hyperparameter allows you to control the nuance you see in the results. This parameter and its behavior will be unique to every dataset you encounter, and depends on the goals of the business. You can iterate on the value of this hyperparameter until you are satisfied with the results, or you can start by leveraging Personalize's hyperparameter optimization (HPO) feature. For more information on hyperparameters and HPO tuning, see the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/customizing-solution-config-hpo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Personalization Recommendations\n",
    "\n",
    "Collaborative filtering-style similar item recommendations are useful, but let's explore something a bit more *personalized*!\n",
    "\n",
    "For this recipe, our input is a **user** ID and the deployed campaign will recommend the movies it thinks are most relevant for that user: Plus some extra recommendations to try and cold-start new items which don't have many interactions yet.\n",
    "\n",
    "Like we took film ID '1' for our first attempt with sims, let's look at the recommendations for user ID '1':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_campaign_arn = get_campaign_arn_by_name(\"personalize-movielens-up\")\n",
    "\n",
    "up_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn=up_campaign_arn,\n",
    "    userId=str(1),\n",
    "    numResults=10,\n",
    ")\n",
    "\n",
    "up_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one difference with this recipe type is that we also receive `score` values for each returned items - so can have an idea of how confident (or not) the model is in its returned results!\n",
    "\n",
    "Let's visualize how different users will receive different recommendations, and compare them to an *anonymous* user by inserting a made-up user ID, `Anonymous`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = [\"Anonymous\"] + list(range(300, 305))\n",
    "\n",
    "up_user_recs = {}\n",
    "n_recs = 10\n",
    "\n",
    "for user_id in user_ids:\n",
    "    user_recs = recs_to_dataframe(\n",
    "        personalize_runtime.get_recommendations(\n",
    "            campaignArn=up_campaign_arn,\n",
    "            userId=str(user_id),\n",
    "            numResults=n_recs,\n",
    "        )[\"itemList\"]\n",
    "    )\n",
    "    up_user_recs[f\"User {user_id}\"] = (\n",
    "        # Need to pad the results to n_recs because some movies might return fewer:\n",
    "        user_recs[\"TITLE\"].to_list() + (n_recs * [None])\n",
    "    )[:n_recs]\n",
    "\n",
    "up_user_recs = pd.DataFrame(up_user_recs)\n",
    "up_user_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that recommendations for the unknown user ID `Anonymous` come back strongly biased towards across-the-board popular films... Whereas our registered users see recommendations more tailored to their viewing (reviewing) history.\n",
    "\n",
    "This is a good start, but what we'd really like is for these recommendations to **update in (near-) real time** as a user interacts with more items on our site!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Filters: \"Sci-Fi Season\"\n",
    "\n",
    "What if we have a special **Sci-Fi season** promotion and we'd like to tweak these same users' recommendations to emphasise our items in the `Sci-Fi` genre category?\n",
    "\n",
    "Luckily, we created a `by-genre` **filter** in the previous notebook! This filter takes a `$GENRE` **parameter**, so we can request which genre we'd like to filter on at run-time.\n",
    "\n",
    "First, we'll need to look up the filter ARN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_arn_by_name(filter_name):\n",
    "    filters = personalize.list_filters(datasetGroupArn=dataset_group_arn)[\"Filters\"]\n",
    "    try:\n",
    "        return next(filter(\n",
    "            lambda f: f[\"name\"] == filter_name,\n",
    "            filters\n",
    "        ))[\"filterArn\"]\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Filter '{}' not found! Got:\\n- {}\".format(\n",
    "            filter_name,\n",
    "            \"\\n- \".join(map(lambda f: f[\"name\"], filters))\n",
    "        ))\n",
    "\n",
    "genre_filter_arn = get_filter_arn_by_name(\"by-genre\")\n",
    "print(genre_filter_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate recommendations as before... But this time specifying our **filter (by ARN)** and **filter variables** (the actual genre we want):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_user_recs = {}\n",
    "\n",
    "for user_id in user_ids:\n",
    "    user_recs = recs_to_dataframe(\n",
    "        personalize_runtime.get_recommendations(\n",
    "            campaignArn=up_campaign_arn,\n",
    "            userId=str(user_id),\n",
    "            numResults=n_recs,\n",
    "            # ADDED:\n",
    "            filterArn=genre_filter_arn,\n",
    "            filterValues={ \"GENRE\": json.dumps(\"Sci-Fi\") }\n",
    "        )[\"itemList\"]\n",
    "    )\n",
    "    up_user_recs[f\"User {user_id}\"] = (\n",
    "        # Need to pad the results to n_recs because some movies might return fewer:\n",
    "        user_recs[\"TITLE\"].to_list() + (n_recs * [None])\n",
    "    )[:n_recs]\n",
    "\n",
    "up_user_recs = pd.DataFrame(up_user_recs)\n",
    "up_user_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some users (looking at you, `303`!) who were already pretty Sci-Fi keen, the recommendations haven't changed very much - but for others, we've made quite a difference!\n",
    "\n",
    "Filtered recommendations can serve many different use-cases such as:\n",
    "\n",
    "- Applying eligibility rules like membership tier, availability, or ratings\n",
    "- Easily creating *shelves* (also known as *rails* or *carousels*) of different item categories with personalized, per-user rankings\n",
    "- Promotional events as suggested above\n",
    "- Removing previously-interacted/purchased items from results (or creating lists specifically targeting repeat purchase)\n",
    "- etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Event Feedback\n",
    "\n",
    "Beyond generating static recommendations on-demand for each user, Personalize has the ability to **listen to events** from your application and **update recommendations** shown to users in near-real-time. In this example we'll focus on injecting **interaction events** (i.e. new clicks/reviews/purchases as a customer move around the site) - but as detailed in the [incremental dataset import documentation](https://docs.aws.amazon.com/personalize/latest/dg/incremental-data-updates.html) there are also APIs available for updating user and item metadata, too.\n",
    "\n",
    "We already created an **event tracker** in the last notebook and kept a record of the `tracking_id`.\n",
    "\n",
    "With this tracking ID, we're able to set up a utility function below that will **log a new interaction** via the **Personalize Events API**.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- In Personalize, user activity is grouped into **sessions**, so we'll just use a simple logic here which creates a new random session ID the first time each `user_id` is used. In real applications, the website's existing session ID system might be used instead.\n",
    "- In an ideal world we will give **not just positive feedback** (interaction events), but also tell the model **what didn't work** by providing `impression` or `recommendation ID` feedback\n",
    "\n",
    "You can find more information about feedback in the [Recording Events](https://docs.aws.amazon.com/personalize/latest/dg/recording-events.html) section of the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_session_id():\n",
    "    return str(uuid.uuid1)\n",
    "\n",
    "# Mapping from user_id to session_id\n",
    "session_dict = defaultdict(generate_random_session_id)\n",
    "\n",
    "def log_new_review(\n",
    "    user_id,\n",
    "    item_id,\n",
    "    rating=5,\n",
    "    recommendation_id=None,\n",
    "    impression=None,\n",
    "    metadata=None,\n",
    "):\n",
    "    \"\"\"Log a new event via Amazon Personalize's Event Tracker\"\"\"\n",
    "    session_id = session_dict[user_id]\n",
    "    event = {\n",
    "        \"eventType\": \"review\",\n",
    "        \"eventValue\": rating,\n",
    "        \"itemId\": str(item_id),\n",
    "        \"sentAt\": datetime.now(),\n",
    "    }\n",
    "    if impression is not None:\n",
    "        # Optionally pass in a list of item ID strings that were presented to the user before\n",
    "        # selecting this one (for negative feedback!)\n",
    "        event[\"impression\"] = impression\n",
    "    if recommendation_id is not None:\n",
    "        # Optionallly pass in the recommendation ID of the list that was generated driving this\n",
    "        # interaction (for implicit feedback!)\n",
    "        event[\"recommendationId\"] = recommendation_id\n",
    "    \n",
    "    personalize_events.put_events(\n",
    "        trackingId=tracking_id,\n",
    "        userId=str(user_id),\n",
    "        sessionId=session_id,\n",
    "        eventList=[event]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine that one of our users who wasn't much of a fan before gets really in to our *Sci-Fi Season* promotion - how might their regular (non-promotional) recommendations change afterwards?\n",
    "\n",
    "In the next cell, we'll choose a particular user ID and:\n",
    "\n",
    "- Generate a list of `n_steps` Sci-Fi films they might watch (using our filtered Sci-Fi recommendations from before)\n",
    "- Fetch their *initial* list of **general** movie recommendations (without the Sci-Fi filter)\n",
    "- Loop through logging new review events for the Sci-Fi films, and seeing how the user's **general** recommendations change after each one\n",
    "\n",
    "We'll display the changing general recommendations as columns of a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 400\n",
    "n_steps = 7\n",
    "\n",
    "# Let's assume they start watching all their personally-recommended Sci-Fi movies, but starting after the Nth\n",
    "# one down the list (since the ones at the very top are probably more generic):\n",
    "n_watchstart = 5\n",
    "# Create the list of movies they'll watch:\n",
    "scifi_recs = recs_to_dataframe(\n",
    "    personalize_runtime.get_recommendations(\n",
    "        campaignArn=up_campaign_arn,\n",
    "        userId=str(user_id),\n",
    "        numResults=n_watchstart + n_steps,\n",
    "        # ADDED:\n",
    "        filterArn=genre_filter_arn,\n",
    "        filterValues={ \"GENRE\": json.dumps(\"Sci-Fi\") }\n",
    "    )[\"itemList\"]\n",
    ").iloc[n_watchstart:]\n",
    "\n",
    "# Now generate their personal recommendations before, and after each Sci-Fi movie watched (reviewed):\n",
    "# Each column in our results table will list recommended titles; with most recent watched movie as the header\n",
    "history = [\n",
    "    recs_to_dataframe(\n",
    "        personalize_runtime.get_recommendations(\n",
    "            campaignArn=up_campaign_arn,\n",
    "            userId=str(user_id),\n",
    "            numResults=n_recs,\n",
    "        )[\"itemList\"]\n",
    "    )[\"TITLE\"].rename(\"Initial Recs\").reset_index(drop=True)\n",
    "]\n",
    "\n",
    "def review_movie_and_update_history(watched_item):\n",
    "    \"\"\"Send movie feedback to Personalize and fetch updated user recommendations\"\"\"\n",
    "    watched_item_id, watched_item_meta = watched_item\n",
    "    watched_item_title = watched_item_meta[\"TITLE\"]\n",
    "\n",
    "    # Record the watched movie:\n",
    "    log_new_review(user_id, watched_item_id)\n",
    "\n",
    "    # Wait a little to help the near-real-time updates propagate:\n",
    "    time.sleep(2.5)\n",
    "\n",
    "    # Generate & record the new personal (non-Sci-Fi) recommendations:\n",
    "    history.append(\n",
    "        recs_to_dataframe(\n",
    "            personalize_runtime.get_recommendations(\n",
    "                campaignArn=up_campaign_arn,\n",
    "                userId=str(user_id),\n",
    "                numResults=n_recs,\n",
    "            )[\"itemList\"]\n",
    "        )[\"TITLE\"].rename(watched_item_title).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# Now loop through the steps with a progress bar:\n",
    "util.progress.notebook_safe_tqdm_loop(\n",
    "    tqdm(scifi_recs.iterrows(), total=n_steps, unit=\"steps\", desc=\"Simulating\"),\n",
    "    review_movie_and_update_history,\n",
    ")\n",
    "\n",
    "history = pd.concat(history, axis=1)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the recommendations **dynamically adapt** as the user watches more movies (the headings) - probably surfacing a few more Sci-Fi films in this user's general recommendations than were present in the initial set.\n",
    "\n",
    "> ⚠️ **Note:** The shifts may not be particularly significant or intuitive in this sample dataset due to the small data volume and limitations discussed when we evaluated our models, but hopefully you still see a few interesting changes!\n",
    "\n",
    "This responsivity is particularly useful in video-on-demand, e-retail, and a whole load of other settings where users may have different **intents** between sessions: Depending whether they're for example watching with children; shopping for something in particular, or so on.\n",
    "\n",
    "As discussed in more detail in [this blog post](https://aws.amazon.com/blogs/machine-learning/amazon-personalize-can-now-create-up-to-50-better-recommendations-for-fast-changing-catalogs-of-new-products-and-fresh-content/), it's **still important to periodically re-train your model**: But this dynamic state provides additional ability to serve your users the right recommendations at the right time.\n",
    "\n",
    "> ⚠️ **Note:** As discussed previously, our sample is a *review* dataset but typical video-on-demand applications would be more likely to deal in \"view\"/\"watch\" events. Just as we experimented with what rating threshold to consider for events, VoD applications may need to consider **how much of the video** a user must have watched before an event is recorded: Sending at 100% complete could miss a lot of people who skip the credits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized Ranking\n",
    "\n",
    "What about use-cases where it's **too difficult to write filtering rules**, and we'd instead like to provide a **shortlist** of items for the model to re-rank in order of relevance for the user? That's the core use case for our *personalized ranking* models.\n",
    "\n",
    "For example you may want to dynamically render a personalized shelf/rail/carousel based on some highly complex criteria such as:\n",
    "\n",
    "- Information that isn't available in your Personalize item metadata (e.g. directior, location, superhero franchise)\n",
    "- Results from some complex upstream short-listing algorithm (like results of a search engine query, kNN or some other machine learning algorithm to generate a cluster of candidate items)\n",
    "- Potentially diverse shortlists that need to be *manually curated* for some other reason.\n",
    "\n",
    "Re-ranking campaigns use a slightly different [GetPersonalizedRanking API](https://docs.aws.amazon.com/personalize/latest/dg/API_RS_GetPersonalizedRanking.html) from the [GetRecommendations](https://docs.aws.amazon.com/personalize/latest/dg/API_RS_GetRecommendations.html) one we've been using so far - but essentially the main difference is just that we need to **supply a list of item IDs** in the request.\n",
    "\n",
    "As an example, let's recommend **Christmas movies**.\n",
    "\n",
    "Our dataset doesn't seem to have any appropriate tags for this in the `GENRES` field, so we can tackle the problem by creating a ranking shortlist by `TITLE` (remember we dropped the `TITLE` field of our item metadata before uploading to Personalize!)\n",
    "\n",
    "Let's first build our Christmas movie shortlist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Of course Die Hard is a Christmas movie!)\n",
    "shortlist_movies_df = items_df[items_df[\"TITLE\"].str.contains(r\"(?:Christmas|Die Hard \\(1988\\))\")]\n",
    "print(f\"Found {len(shortlist_movies_df)} matching movies. Sample:\")\n",
    "shortlist_movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're all set to generate our personalized seasonal carousels: Even applying additional filtering criteria, if we want.\n",
    "\n",
    "Let's explore the holiday picks for a set of example users, **filtering out any they might have reviewed before**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_campaign_arn = get_campaign_arn_by_name(\"personalize-movielens-rerank\")\n",
    "unwatched_filter_arn = get_filter_arn_by_name(\"unwatched\")\n",
    "\n",
    "# Convert the movie shortlist to just a list of (up to 500) ITEM_ID strings:\n",
    "shortlist_item_ids = shortlist_movies_df.index.astype(str).to_list()[:500]\n",
    "\n",
    "rerank_user_recs = {}\n",
    "\n",
    "for user_id in user_ids:\n",
    "    user_recs = recs_to_dataframe(\n",
    "        personalize_runtime.get_personalized_ranking(\n",
    "            campaignArn=rerank_campaign_arn,\n",
    "            userId=str(user_id),\n",
    "            inputList=shortlist_item_ids,\n",
    "            filterArn=unwatched_filter_arn,\n",
    "        )[\"personalizedRanking\"]\n",
    "    )\n",
    "    rerank_user_recs[f\"User {user_id}\"] = (\n",
    "        # Need to pad the results to n_recs in case some users might return fewer:\n",
    "        user_recs[\"TITLE\"].to_list() + (n_recs * [None])\n",
    "    )[:n_recs]\n",
    "\n",
    "rerank_user_recs = pd.DataFrame(rerank_user_recs)\n",
    "rerank_user_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and so we can rank arbitrary collections of items - even if there's no nice way to express those collections as filter rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Recommendations\n",
    "\n",
    "Although not the starting point for most projects, there are many cases where you may want to build a bulk dataset of exported recommendations.\n",
    "\n",
    "Here we'll give a quick walkthrough of the process for the User-Personalization recipe via the Python SDK; although of course it's also possible:\n",
    "\n",
    "- ...Through the Amazon Personalize console UI (see the *Batch inference jobs* tab of the sidebar within your dataset group)\n",
    "- ...For other recipe types as well (although the output format will vary a little)\n",
    "\n",
    "You can find more information in the [Getting Batch Recommendations](https://docs.aws.amazon.com/personalize/latest/dg/recommendations-batch.html) section of the developer guide.\n",
    "\n",
    "### Building the input file\n",
    "\n",
    "To use the batch inference feature, you specify the inputs that you'd like to generate recommendations for up-front. Since the input fields differ between different **recipe types**, the exact format of the input file will be different too.\n",
    "\n",
    "For our standard user personalization use-case, we'll need a [JSON-Lines](https://jsonlines.org/) file specifying just the `USER_ID` for each request, something like this:\n",
    "\n",
    "```json\n",
    "{\"userId\": \"4638\"}\n",
    "{\"userId\": \"663\"}\n",
    "{\"userId\": \"3384\"}\n",
    "```\n",
    "\n",
    "The cell below will again select a few candidate users and create an input file here on the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_filename = \"batch_up_input.json\"\n",
    "batch_input_path = f\"{data_dir}/{batch_input_filename}\"\n",
    "\n",
    "with open(batch_input_path, \"w\") as f:\n",
    "    for user_id in range(1, 50):\n",
    "        f.write(json.dumps({ \"userId\": str(user_id) }) + \"\\n\")\n",
    "\n",
    "print(f\"Written input to {batch_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can open the above file in the notebook to check the format is as expected)\n",
    "\n",
    "As usual when working with Personalize, we'll need to upload that input to an Amazon S3 bucket - we'll use the same one as created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to S3\n",
    "boto3.resource(\"s3\").Bucket(bucket_name).Object(batch_input_path).upload_file(batch_input_path)\n",
    "batch_input_s3uri = f\"s3://{bucket_name}/{batch_input_path}\"\n",
    "print(f\"Uploaded:\\n{batch_input_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Job\n",
    "\n",
    "With our input file prepared, the other parameters required to create a batch inference job are not so different from what we've used already so far. One major difference is that we supply a **solution version, not a campaign**: Because we don't need to have deployed our model to a real-time endpoint, to create batch recommendations with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need a unique job name:\n",
    "batch_job_name = f\"personalize-movielens-up-batch-{str(round(time.time()*1000))}\"\n",
    "\n",
    "create_batch_job_resp = personalize.create_batch_inference_job(\n",
    "    # Point to our trained solution version (model):\n",
    "    solutionVersionArn=up_solution_version_arn,\n",
    "    jobName=batch_job_name,\n",
    "    # An IAM role authorizing Personalize to access the S3 source & target:\n",
    "    roleArn=personalize_role_arn,\n",
    "    # Input and output data locations in S3:\n",
    "    jobInput={ \"s3DataSource\": { \"path\": batch_input_s3uri } },\n",
    "    jobOutput = { \"s3DataDestination\": {\n",
    "        \"path\": f\"s3://{export_bucket_name}/batch-results/{batch_job_name}/\",\n",
    "    } }\n",
    ")\n",
    "\n",
    "batch_job_arn = create_batch_job_resp[\"batchInferenceJobArn\"]\n",
    "%store batch_job_arn\n",
    "create_batch_job_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And just like you might expect from our experience with dataset import jobs - this process is kicked off **in the background**\n",
    "\n",
    "> ⏰ This batch inference job can take around 30 minutes to complete, and as we previously saw with dataset import jobs - that time is typically dominated by infrastructure provisioning and setup overheads for this small sample dataset. More typical bulk processing use-cases will see much more efficiency!\n",
    "\n",
    "The cell below will set up a polling loop to wait for the batch job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_batch_job_finished(desc):\n",
    "    status = desc[\"batchInferenceJob\"][\"status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(f\"Batch job failed!\\n{desc}\")\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=lambda: personalize.describe_batch_inference_job(\n",
    "        batchInferenceJobArn=batch_job_arn,\n",
    "    ),\n",
    "    fn_is_finished=is_batch_job_finished,\n",
    "    fn_stringify_result=lambda d: d[\"batchInferenceJob\"][\"status\"],\n",
    "    poll_secs=30,\n",
    "    timeout_secs=60*60,  # Max 1 hour\n",
    ")\n",
    "print(\"Batch inference job complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Results\n",
    "\n",
    "Like the input, the format of our outputs will differ a little between recipe types as described in the documentation. For this example in user personalization, we can expect to see JSON-Lines file(s) with a structure something like the below:\n",
    "\n",
    "```json\n",
    "{\"input\":{\"userId\":\"4638\"}, \"output\": {\"recommendedItems\": [\"296\", \"1\", \"260\", \"318\"]}}\n",
    "{\"input\":{\"userId\":\"663\"}, \"output\": {\"recommendedItems\": [\"1393\", \"3793\", \"2701\", \"3826\"]}}\n",
    "{\"input\":{\"userId\":\"3384\"}, \"output\": {\"recommendedItems\": [\"8368\", \"5989\", \"40815\", \"48780\"]}}\n",
    "```\n",
    "\n",
    "The `output` keys here correspond quite closely to the structure of real-time API responses. We can download our files from Amazon S3 and inspect the structure to confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the output S3 URI from the job description:\n",
    "batch_job_desc = personalize.describe_batch_inference_job(\n",
    "    batchInferenceJobArn=batch_job_arn,\n",
    ")[\"batchInferenceJob\"]\n",
    "\n",
    "batch_output_s3uri = batch_job_desc[\"jobOutput\"][\"s3DataDestination\"][\"path\"]\n",
    "\n",
    "# Use the job name to build a local folder to store the output:\n",
    "batch_output_path = f\"{data_dir}/batch-results/{batch_job_desc['jobName']}\"\n",
    "\n",
    "# Download the outputs from S3 to local folder:\n",
    "!aws s3 sync $batch_output_s3uri $batch_output_path\n",
    "print(\"\\nDownload finished!\")\n",
    "\n",
    "for filename in filter(lambda f: \".json\" in f, os.listdir(batch_output_path)):\n",
    "    print(f\"\\n>\\tSAMPLE of {filename}:\")\n",
    "    with open(os.path.join(batch_output_path, filename), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(\"\".join(lines[:3]))\n",
    "        if len(lines) > 3:\n",
    "            print(\"\\t...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then using the recommendations is simply a case of reading the file line-by-line and processing the item IDs for each user however your use case requires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "With that you now have a fully working collection of models to tackle various recommendation and personalization scenarios, as well as the skills to manipulate data to better integrate with the service.\n",
    "\n",
    "You'll want to make sure that you clean up all of the resources deployed during this PoC, to avoid potential ongoing charges (particularly for deployed infrastructure such as campaigns). We have provided a separate notebook which shows you how to identify and delete the resources in [06_Clean_Up_Resources.ipynb](06_Clean_Up_Resources.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
