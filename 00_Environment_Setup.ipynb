{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Amazon Personalize will access source data from (and optionally export batch recommendations to) Amazon S3... So before we start using Personalize, we should set up our bucket(s) and permissions.\n",
    "\n",
    "Production environments will typically automate this setup via tools like [AWS CloudFormation](https://aws.amazon.com/cloudformation/) and the [AWS Cloud Development Kit](https://aws.amazon.com/cdk/).\n",
    "\n",
    "Since we're just experimenting, we'll instead use this notebook to keep the setup easily customizable for your environment. (Assuming you're running the notebook with appropriate IAM and S3 administrative permissions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # (AWS Python SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping this Notebook\n",
    "\n",
    "**If** you have buckets and permissions set up already, or plan to work through the following steps in the AWS Console instead of running the Python code - you'll need to **store** your setup to work with the rest of the notebooks in this series.\n",
    "\n",
    "Simply un-comment the code below (can select all the contents of the cell and press `Control`+`/`), replace the placeholder values with your own, and then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = \"ap-southeast-1\"  # or \"us-east-1\", etc etc: Whichever AWS region you're working in\n",
    "# %store region\n",
    "# bucket_name = \"DOC-EXAMPLE-BUCKET\"  # Whatever you named your data bucket\n",
    "# %store bucket_name\n",
    "# export_bucket_name = bucket_name  # (Assuming you want to export results to the same bucket?)\n",
    "# %store export_bucket_name\n",
    "# personalize_role_name = \"PersonalizeRolePOC\"\n",
    "# %store personalize_role_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to an AWS Region\n",
    "\n",
    "Assuming you're running this notebook on [Amazon SageMaker](https://aws.amazon.com/sagemaker/), it will already be associated with a particular [AWS Region](https://aws.amazon.com/about-aws/global-infrastructure/) and be running with certain [AWS IAM Permissions](https://aws.amazon.com/iam/) (defined by the **notebook execution role**).\n",
    "\n",
    "If you're running the notebook locally, you may need to explicitly log in e.g. using `aws configure` from the [AWS CLI](https://aws.amazon.com/cli/), and set the specific region you'd like to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=None)  # To set a specific region, replace None with e.g. \"us-east-1\"\n",
    "region = session.region_name  # We'll save the configured region to initialize later notebooks\n",
    "print(region)\n",
    "%store region\n",
    "\n",
    "iam = session.client(\"iam\")\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket(s)\n",
    "\n",
    "Amazon Personalize will read historical data from S3, and may export batch recommendations to S3.\n",
    "\n",
    "By default, we'll create a single bucket for both with a partially-randomized name (since S3 bucket names must be globally unique).\n",
    "\n",
    "You can customize this setup (e.g. to use an existing bucket instead) and/or configure through the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/home).\n",
    "\n",
    "Just be sure to `%store` a valid `bucket_name` and `export_bucket_name` which exist in the same `region`: We'll use this below and in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a source data bucket name:\n",
    "bucket_name = \"{}-{}-personalizepocvod\".format(\n",
    "    session.client(\"sts\").get_caller_identity().get(\"Account\"),\n",
    "    region,\n",
    ")\n",
    "print(bucket_name)\n",
    "%store bucket_name\n",
    "\n",
    "# Create the bucket (assuming it's new):\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ \"LocationConstraint\": region })\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume any exports can go in the same bucket:\n",
    "\n",
    "export_bucket_name = bucket_name\n",
    "%store export_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket Policies\n",
    "\n",
    "To access data in these buckets, Amazon Personalize needs permissions. This means creating **both**:\n",
    "\n",
    "- An **execution role** with appropriate permissions to grant access to individual **import jobs** running with it, and\n",
    "- A **bucket policy** to allow the Amazon Personalize **service** access in the first place\n",
    "\n",
    "Below, we'll set up bucket policies for the buckets set up above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in set((bucket_name, export_bucket_name)):\n",
    "    policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"personalize.amazonaws.com\",\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"s3:*Object\",\n",
    "                    \"s3:ListBucket\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket}\",\n",
    "                    f\"arn:aws:s3:::{bucket}/*\",\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    s3.BucketPolicy(bucket).put(Policy=json.dumps(policy))\n",
    "    print(f\"Added policy to {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Role for Personalize\n",
    "\n",
    "To access data in these buckets, Amazon Personalize needs permissions. This means creating a **role** with appropriate access the buckets and which can be assumed by the Personalize service.\n",
    "\n",
    "By default, we'll create a new role and attach necessary permissions here. You can customize this setup and/or configure through the [AWS IAM Console](https://console.aws.amazon.com/iam/home).\n",
    "\n",
    "Just be sure to `%store` a valid `personalize_role_arn`: We'll use this in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize_role_name = \"PersonalizeRolePOC\"\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=personalize_role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"personalize.amazonaws.com\",\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "            },\n",
    "        ]\n",
    "    }),\n",
    ")\n",
    "\n",
    "personalize_role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(personalize_role_arn)\n",
    "%store personalize_role_arn\n",
    "\n",
    "# Note that AmazonPersonalizeFullAccess provides access to some specifically-named default S3 buckets as well,\n",
    "# but we just want it for the Forecast permissions themselves:\n",
    "iam.attach_role_policy(\n",
    "    RoleName=personalize_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\",\n",
    ")\n",
    "\n",
    "# By default (since we're experimenting), this code attaches over-generous S3 permissions (full access):\n",
    "iam.attach_role_policy(\n",
    "    RoleName=personalize_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    ")\n",
    "# You could instead use something like the below to give access to *only* the relevant buckets:\n",
    "# inline_s3_policy = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": \"s3:*\",\n",
    "#             \"Resource\": [\n",
    "#                 # (Assuming you're not running in a different partition e.g. aws-cn)\n",
    "#                 f\"arn:aws:s3:::{bucket_name}\",\n",
    "#                 f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "#             ]\n",
    "#         },\n",
    "#     ],\n",
    "# }\n",
    "# if bucket_name != export_bucket_name:\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}\")\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}/*\")\n",
    "\n",
    "# iam.put_role_policy(\n",
    "#     RoleName=personalize_role_name,\n",
    "#     PolicyName=\"PersonalizePoCBucketAccess\",\n",
    "#     PolicyDocument=json.dumps(inline_s3_policy)\n",
    "# )\n",
    "\n",
    "# IAM policy attachments *may* take up to a minute to propagate, so just to be safe:\n",
    "sleep(60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you get an iam.exceptions.EntityAlreadyExistsException on the role, can instead:\n",
    "# personalize_role_arn = iam.get_role(RoleName=personalize_role_name)[\"Role\"][\"Arn\"]\n",
    "# print(personalize_role_arn)\n",
    "# %store personalize_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All set!\n",
    "\n",
    "Your environment should now be all set up with:\n",
    "\n",
    "- S3 bucket(s) `bucket_name` and `export_bucket_name` for importing source data to Amazon Personalize, and optionally exporting batch recommendations\n",
    "- An IAM role `personalize_role_arn` granting Amazon Personalize permission to interact with those buckets\n",
    "\n",
    "You're now ready to move on to the next stage: Preparing input data for the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
